# Project Goals & Scope Statement
This research project seeks to repurpose the ScorBot ER-4, an educational robotic arm, into an autonomous robotic cameraperson. While basic videography techniques are well understood, capturing high-quality footage in dynamic environments like classrooms or live presentations requires constant operator attention. The project's goal is to automate camera operations—such as panning, tilting, zooming, and framing subjects—to create more engaging recordings. This project includes the ideation, research, and development of software that integrates machine learning, computer vision, and other technologies to guide camera movements. Secondary objectives include multi-cameras for A and B-Roll, multi-subject tracking, and live or post-processed video footage. The system’s scope and features will evolve as the capabilities of the software and hardware become clearer. A core goal of the project is to extend the life of outdated technology by combining modern cameras with older robotics arms for new applications. 

Currently, scope includes: 
- 2 arms that each have their own cameras that work in tandem
- Facial tracking AI algorithms to the camera-arms to follow speakers
- Camera position saving to lock a view onto a deak ot white board
- Automated camera switching for optimal viewing pleasure

# Planned Milestones & Major Deliverables
1. MVP with 1 arm moving 1 camera controlled either by manual controls or an AI facial tracking algorithm

2. 2 arms moving 2 cameras with the same features at the MVP

3. 2 arms moving 2 cameras, where the operator can setup 1 camera to hold a position and the other to follow facial. 

# Initial requirements & User Stories

The main user profile for this camera system is a high-skill lecturing professor who has need for a camera in the classroom either for online students or future recording purposes. All of the requirements for the project stem from this user profile and their needs. 

## Professor User Stories
- As a professor, I want to turn on/off the system so that I can use the autonomous camera features. 
- As a professor, I want to enable either the AI-controlled or manual mode so that I have control and know whether I cam controlling the camera or the AI is controlling the camera
- As a professor, I want to walk around the classroom and have the camera follow me so that I can lecture and do not have to move the camera at the same time. 
- As a professor, I want to enable different AI camera behaviors so that when I lecture in different environments and different circumstances, the camera can behave differently. 
- As a professor, I want to manually change where the camera is pointing so that if the AI is not moving how I want it to, I can adjust it to where I need it. 

# Work Breakdown & Subteams
## AI and Camera Team
Nolan & John

## Embedded Team
Brooke & Noah

## Networking Team
Alex & Devan

# Communication and Stakeholder Management Plan
Every Tuesday, we meet with our sponsor/coach, giving status updates to our project and collecting feedback about potential features for the future of the project. Additionaly, any concerns about major risks of the project, given the nature of its reserach-oriented development, are brought up during the meeting and discussed to resolve how to handle the risk. 

As part of the weekly meetings, 4-Up document is presented to the sponsor, detailing the progress of the teams, the current risks of the project, the near future plans for the team, and the needs of the team from the sponsor/coach. 

# Risk Management Plan
A running risk register is created to track the risks of highest expected impact on the team and the project. As part of the weekly meeting with the sponsor/coach, updates to impactful/notable movements on the risk register are addressed and either resolved or a plan to anticipate those risks is made. 
